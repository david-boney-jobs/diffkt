{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# **DiffKT** Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Copyright (c) Meta Platforms, Inc. and affiliates.**\n",
    " \n",
    " This source code is licensed under the MIT license found in the\n",
    " LICENSE file in the root directory of this source tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook discusses the model api, which is in diffkt/kotlin/api/src/main/kotlin/org/diffkt/model.\n",
    "The model api is used to build deep neural networks using the automatic differentiation in **DiffKt**.\n",
    "This notebook will use a simple linear regression example to show how to use the model api. This notebook is based on the example __[Linear Regression](https://github.com/facebookresearch/diffkt/tree/main/kotlin/examples/src/main/kotlin/examples/linreg)__.\n",
    "\n",
    "There are additional examples using the model api:\n",
    "\n",
    "__[Iris](https://github.com/facebookresearch/diffkt/tree/main/kotlin/examples/src/main/kotlin/examples/iris)__, an image processing example using dense layers in a neural network.\n",
    "\n",
    "__[MNIST](https://github.com/facebookresearch/diffkt/tree/main/kotlin/examples/src/main/kotlin/examples/mnist)__, an image processing example using a convolution neural network.\n",
    "\n",
    "__[RESNET](https://github.com/facebookresearch/diffkt/tree/main/kotlin/examples/src/main/kotlin/examples/resnet)__, an image processing example using a deep convolution neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Housekeeping\n",
    "\n",
    "The following jars need to be included in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@file:DependsOn(\"../kotlin/api/build/libs/api.jar\")\n",
    "@file:DependsOn(\"../kotlin/data/build/libs/data.jar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Import the following classes for the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import org.diffkt.*\n",
    "import org.diffkt.data.Data\n",
    "import org.diffkt.model.*\n",
    "import org.diffkt.tracing.jit\n",
    "import kotlin.math.min\n",
    "import kotlin.random.Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "This is a simple linear regression model of $y = ax + b$, where\n",
    "\n",
    "$x$ - feature or input \n",
    "\n",
    "$y$ - label or  output\n",
    "\n",
    "$a$ - the weight\n",
    "\n",
    "$b$ - the bias\n",
    "\n",
    "The goal of linear regression is to recover the weight and the bias given the model, the input data, and the output data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The Model API\n",
    "\n",
    "A number of steps are required to use the Model API.\n",
    "\n",
    "1) Create some training data to use to build the model,\n",
    "\n",
    "2) Create an interator over the data for training the model,\n",
    "\n",
    "3) Create a linear regression model that inherits from the Model API,\n",
    "\n",
    "4) Create a loss function,\n",
    "\n",
    "5) Create an optimizer,\n",
    "\n",
    "6) Create a learning class,\n",
    "\n",
    "7) Train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Setup\n",
    "\n",
    "The training data set size is 100 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Setup\n",
    "\n",
    "val trainingDataSize = 100\n",
    "val random = Random(1234567)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data\n",
    "\n",
    "The function `makeTrainingData()` creates a vector, or a 1D tensor, of 100 random inputs, or features. Tensor based arithmatic is used to create the labels vector, where $labels = features * trueWeight + trueBias$. The features, labels, trueWeight, and trueBias are return in an object of class `TrainingData`. trueWeight and trueBias are stored with the data so we can see how accurate a model is produced from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Training Data\n",
    "\n",
    "class TrainingData(val features : FloatTensor, \n",
    "                   val labels : FloatTensor, \n",
    "                   val trueWeight : FloatScalar, \n",
    "                   val trueBias : FloatScalar) {\n",
    "       \n",
    "    companion object {\n",
    "              \n",
    "        fun makeTrainingData(trainingDataSize: Int, random : Random ) : TrainingData {    \n",
    "\n",
    "            val trueWeight = FloatScalar(random.nextFloat())\n",
    "            val trueBias = FloatScalar(random.nextFloat())\n",
    "            \n",
    "            val features = FloatTensor(Shape(trainingDataSize)) { random.nextFloat() }\n",
    "            val labels = (features * trueWeight + trueBias) as FloatTensor\n",
    "        \n",
    "            return TrainingData(features, labels, trueWeight, trueBias)\n",
    "        }\n",
    "    }\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val trainingData = TrainingData.makeTrainingData(trainingDataSize, random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Iterator\n",
    "\n",
    "The `SimpleDataIterator` class creates an iterator over class `Data`. Class `Data` is located in __[diffkt/kotlin/data/src/main/kotlin/org/diffkt/data/Data.kt](https://github.com/facebookresearch/diffkt/blob/main/kotlin/data/src/main/kotlin/org/diffkt/data/Data.kt)__. It can hold the labels and features for a training set and provides an iterator over the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleDataIterator(val features: FloatTensor,\n",
    "                         val labels: FloatTensor,\n",
    "                         val batchSize: Int = 1): Iterable<Data> {\n",
    "    \n",
    "    init {\n",
    "        require(features.shape.first == labels.shape.first)\n",
    "    }\n",
    "\n",
    "    private val n = features.shape.first\n",
    "\n",
    "    fun withBatchSize(batchSize: Int) = SimpleDataIterator(features, labels, batchSize)\n",
    "\n",
    "    override fun iterator(): Iterator<Data> = object : Iterator<Data> {\n",
    "        var loc = 0\n",
    "        override fun hasNext(): Boolean = loc < n\n",
    "        override fun next(): Data {\n",
    "            require(hasNext())\n",
    "\n",
    "            val start = loc\n",
    "            val end = min(loc + batchSize, n)\n",
    "            val f = features.slice(start, end)\n",
    "            val l = labels.slice(start, end)\n",
    "            loc = end\n",
    "            return Data(f, l)\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Data Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dataIterator = SimpleDataIterator(trainingData.features, trainingData.labels, trainingDataSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LinearRegression(val l: AffineTransform): Model<LinearRegression>() {\n",
    "    \n",
    "    constructor(m: DScalar, b: DScalar) : this(AffineTransform(TrainableTensor(m), TrainableTensor(b)))\n",
    "    constructor(random: Random) : this(FloatScalar(random.nextFloat()), FloatScalar(random.nextFloat()))\n",
    "\n",
    "    override val layers: List<Layer<*>> = listOf(l)\n",
    "\n",
    "    override fun withLayers(newLayers: List<Layer<*>>): LinearRegression {\n",
    "        require(newLayers.size == 1)\n",
    "        val newLayer = newLayers[0] as AffineTransform\n",
    "        return LinearRegression(newLayer)\n",
    "    }\n",
    "\n",
    "    override fun hashCode(): Int = combineHash(\"LinearRegression\", l)\n",
    "    override fun equals(other: Any?): Boolean = other is LinearRegression &&\n",
    "            other.l == l\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val linReg = LinearRegression(random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun lossFun(predictions: DTensor, labels: DTensor): DScalar {\n",
    "    val diff = predictions - labels\n",
    "    return (diff * diff).sum()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val optimizer = FixedLearningRateOptimizer<LinearRegression>(0.5F / trainingDataSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Learner<T : Model<T>>(val batchedData: Iterable<Data>,\n",
    "                            val lossFunc: (predictions: DTensor, labels: DTensor) -> DScalar,\n",
    "                            val optimizer: Optimizer<T> = AdamOptimizer(),\n",
    "                            val useJit: Boolean = false) \n",
    "{\n",
    "    var totalTime = 0L\n",
    "\n",
    "    /**\n",
    "     * Trains the given model on the data set, for [epochs] epochs processing the data of the [dataIterator] in\n",
    "     * batches of size [batchSize], but with a maximum number of total batches processed [maxIters].  Returns\n",
    "     * the trained model.\n",
    "     */\n",
    "    fun train(model: T,\n",
    "              epochs: Int,\n",
    "              printProgress: Boolean = false,\n",
    "              maxIters: Int? = null,\n",
    "              printProgressFrequently: Boolean = false,\n",
    "              device: Device = Device.CPU): T \n",
    "    {\n",
    "        \n",
    "        var totalIters = 0\n",
    "\n",
    "        // The model training step function, which could possibly be optimized.\n",
    "        fun modelTrainStep(model2: T, batch: Data): Pair<DScalar, T> \n",
    "        {\n",
    "            val (loss, tangent) = primalAndReverseDerivative(\n",
    "                x = model2,\n",
    "                f = { model3: T ->\n",
    "                    val output = model3.predict(batch.features)\n",
    "                    val loss = lossFunc(output, batch.labels)\n",
    "                    loss\n",
    "                },\n",
    "                extractDerivative = { model3: T,\n",
    "                                      loss: DScalar,\n",
    "                                      extractor: (input: DTensor, output: DTensor) -> DTensor ->\n",
    "                    model3.extractTangent(loss, extractor)\n",
    "                }\n",
    "            )\n",
    "\n",
    "            val trainedModel: T = optimizer.train(model2, tangent)\n",
    "            return Pair(loss, trainedModel)\n",
    "        }\n",
    "        \n",
    "        fun trainingFunction(p: Pair<T, Data>): Pair<DScalar, T> = modelTrainStep(p.first, p.second)\n",
    "        \n",
    "        val jittedTrainingFunction = if (useJit) jit(::trainingFunction) else ::trainingFunction\n",
    "\n",
    "        val optimizedModel = (0 until epochs).fold(model) { model1: T, e: Int ->\n",
    "            var lossTotal: DScalar = FloatScalar.ZERO\n",
    "            val trainedModel = batchedData.fold(model1) { model2: T, batch: Data ->\n",
    "                val batchOnDevice = batch.to(device)\n",
    "                val t1 = System.nanoTime()\n",
    "                val (loss, trainedModel) = jittedTrainingFunction(Pair(model2, batchOnDevice))\n",
    "                val t2 = System.nanoTime()\n",
    "                totalTime += t2 - t1\n",
    "                if (printProgress) lossTotal += loss\n",
    "                totalIters++\n",
    "                if (printProgressFrequently) println(\"Iter $totalIters Batch Loss: $loss\")\n",
    "                if (maxIters != null && totalIters >= maxIters) return trainedModel\n",
    "                trainedModel\n",
    "            }\n",
    "            if (printProgress && ((e % 10) == 0)) println(\"Epoch $e Cumulative Loss: $lossTotal\")\n",
    "            trainedModel\n",
    "        }\n",
    "        \n",
    "        return optimizedModel\n",
    "    }\n",
    "\n",
    "    private fun e(n: Long) = n / 1e9f\n",
    "\n",
    "    fun dumpTimes() {\n",
    "        println(\"running time:  ${e(totalTime)} sec\")\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Cumulative Loss: 68.87204\n",
      "Epoch 10 Cumulative Loss: 0.06945361\n",
      "Epoch 20 Cumulative Loss: 0.02103138\n",
      "Epoch 30 Cumulative Loss: 0.0063685565\n",
      "Epoch 40 Cumulative Loss: 0.0019284704\n",
      "Epoch 50 Cumulative Loss: 5.839721E-4\n",
      "Epoch 60 Cumulative Loss: 1.768291E-4\n",
      "Epoch 70 Cumulative Loss: 5.3545296E-5\n",
      "Epoch 80 Cumulative Loss: 1.6213318E-5\n",
      "Epoch 90 Cumulative Loss: 4.909573E-6\n",
      "Epoch 100 Cumulative Loss: 1.4866098E-6\n",
      "Epoch 110 Cumulative Loss: 4.5023984E-7\n",
      "Epoch 120 Cumulative Loss: 1.3625511E-7\n",
      "Epoch 130 Cumulative Loss: 4.1285187E-8\n",
      "Epoch 140 Cumulative Loss: 1.2533533E-8\n",
      "Epoch 150 Cumulative Loss: 3.7813876E-9\n",
      "Epoch 160 Cumulative Loss: 1.1489654E-9\n",
      "Epoch 170 Cumulative Loss: 3.4492587E-10\n",
      "Epoch 180 Cumulative Loss: 1.0542678E-10\n",
      "Epoch 190 Cumulative Loss: 3.381828E-11\n",
      "Epoch 200 Cumulative Loss: 1.0352608E-11\n",
      "Epoch 210 Cumulative Loss: 3.2045477E-12\n",
      "Epoch 220 Cumulative Loss: 2.5330849E-12\n",
      "Epoch 230 Cumulative Loss: 2.5330849E-12\n",
      "Epoch 240 Cumulative Loss: 2.5330849E-12\n",
      "Epoch 250 Cumulative Loss: 2.5330849E-12\n",
      "Epoch 260 Cumulative Loss: 2.5330849E-12\n",
      "Epoch 270 Cumulative Loss: 2.5330849E-12\n",
      "Epoch 280 Cumulative Loss: 2.5330849E-12\n",
      "Epoch 290 Cumulative Loss: 2.5330849E-12\n",
      "running time:  1.0420661 sec\n"
     ]
    }
   ],
   "source": [
    "val learner = Learner(batchedData = dataIterator,\n",
    "                      lossFunc = ::lossFun,\n",
    "                      optimizer = optimizer,\n",
    "                      useJit = true)\n",
    "\n",
    "val trainedModel = learner.train(linReg, 300, printProgress = true)\n",
    "learner.dumpTimes()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "nbconvert_exporter": "",
   "pygments_lexer": "kotlin",
   "version": "1.7.20-dev-1299"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
