{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# **DiffKT** Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Copyright (c) Meta Platforms, Inc. and affiliates.**\n",
    " \n",
    " This source code is licensed under the MIT license found in the\n",
    " LICENSE file in the root directory of this source tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook discusses the model api, which is in __[diffkt/kotlin/api/src/main/kotlin/org/diffkt/model](https://github.com/facebookresearch/diffkt/tree/main/kotlin/api/src/main/kotlin/org/diffkt/model)__.\n",
    "The model api is used to build deep neural networks using the automatic differentiation in **DiffKt**.\n",
    "This notebook will use a simple linear regression example to show how to use the model api. This notebook is based on the example __[Linear Regression](https://github.com/facebookresearch/diffkt/tree/main/kotlin/examples/src/main/kotlin/examples/linreg)__.\n",
    "\n",
    "There are additional examples using the model api:\n",
    "\n",
    "__[Iris](https://github.com/facebookresearch/diffkt/tree/main/kotlin/examples/src/main/kotlin/examples/iris)__, an image processing example using dense layers in a neural network.\n",
    "\n",
    "__[MNIST](https://github.com/facebookresearch/diffkt/tree/main/kotlin/examples/src/main/kotlin/examples/mnist)__, an image processing example using a convolution neural network.\n",
    "\n",
    "__[RESNET](https://github.com/facebookresearch/diffkt/tree/main/kotlin/examples/src/main/kotlin/examples/resnet)__, an image processing example using a deep convolution neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Housekeeping\n",
    "\n",
    "The following jars need to be included in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@file:DependsOn(\"../kotlin/api/build/libs/api.jar\")\n",
    "@file:DependsOn(\"../kotlin/data/build/libs/data.jar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Import the following classes for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import org.diffkt.*\n",
    "import org.diffkt.data.Data\n",
    "import org.diffkt.model.*\n",
    "import org.diffkt.tracing.jit\n",
    "import kotlin.math.min\n",
    "import kotlin.random.Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "This notebook implements a simple linear regression model of $labels = m * features + b$, where\n",
    "\n",
    "$labels$ - output\n",
    "\n",
    "$features$ - input\n",
    "\n",
    "$m$ - the weight\n",
    "\n",
    "$b$ - the bias\n",
    "\n",
    "For this notebook, the model that generates the data is $labels = 5 * features + 2$\n",
    "\n",
    "The goal of linear regression is to recover the weight and the bias given the model, the input data, and the output data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The Model API\n",
    "\n",
    "The `Model` API uses gradient descent to estimate the coefficients for the linear regression model. The linear regression model uses a single layer of a neural network.\n",
    "\n",
    "A number of steps are required to use the Model API.\n",
    "\n",
    "1) Create some training data to use to build the model,\n",
    "\n",
    "2) Create an interator over the data for training the model,\n",
    "\n",
    "3) Create a linear regression model that inherits from the Model API,\n",
    "\n",
    "4) Create a loss function,\n",
    "\n",
    "5) Create an optimizer,\n",
    "\n",
    "6) Create a learning class,\n",
    "\n",
    "7) Train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Setup\n",
    "\n",
    "The training data set size is 100 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Setup\n",
    "\n",
    "val trainingDataSize = 100\n",
    "val random = Random(1234567)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data\n",
    "\n",
    "The function `makeTrainingData()` creates a vector, or a 1D tensor, of 100 random inputs, or features. Tensor based arithmatic is used to create the labels vector, where $labels = features * trueWeight + trueBias$. The $trueWeight = 5.0$ and the $trueBias = 2.0$, so the model to be learned will be $labels = 5.0 * features + 2.0$ The features, labels, trueWeight, and trueBias are return in an object of class `TrainingData`. The `trueWeight` and `trueBias` are stored with the data so we can see how accurate a model is produced from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Training Data\n",
    "\n",
    "class TrainingData(val features : FloatTensor, \n",
    "                   val labels : FloatTensor, \n",
    "                   val trueWeight : FloatScalar, \n",
    "                   val trueBias : FloatScalar) {\n",
    "       \n",
    "    companion object {\n",
    "              \n",
    "        fun makeTrainingData(trainingDataSize: Int, random : Random ) : TrainingData {    \n",
    "\n",
    "            val trueWeight = FloatScalar(5f)\n",
    "            val trueBias = FloatScalar(2f)\n",
    "            \n",
    "            val features = FloatTensor(Shape(trainingDataSize)) { random.nextFloat() }\n",
    "            val labels = (features * trueWeight + trueBias) as FloatTensor\n",
    "        \n",
    "            return TrainingData(features, labels, trueWeight, trueBias)\n",
    "        }\n",
    "    }\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val trainingData = TrainingData.makeTrainingData(trainingDataSize, random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Iterator\n",
    "\n",
    "The `SimpleDataIterator` class creates an iterator over class `Data`. Class `Data` is located in __[diffkt/kotlin/data/src/main/kotlin/org/diffkt/data/Data.kt](https://github.com/facebookresearch/diffkt/blob/main/kotlin/data/src/main/kotlin/org/diffkt/data/Data.kt)__. Class `Data` holds the labels and features for a training set and provides an iterator over the data to provide data to the learning algorithm in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "// Data Iterator\n",
    "\n",
    "class SimpleDataIterator(val features: FloatTensor,\n",
    "                         val labels: FloatTensor,\n",
    "                         val batchSize: Int = 1): Iterable<Data> {\n",
    "\n",
    "    // you need the same number of labels as features\n",
    "    init {\n",
    "        require(features.shape.first == labels.shape.first)\n",
    "    }\n",
    "\n",
    "    // number of training examples\n",
    "    private val n = features.shape.first\n",
    "\n",
    "    fun withBatchSize(batchSize: Int) = SimpleDataIterator(features, labels, batchSize)\n",
    "\n",
    "    // gets a slide of data from the tensors\n",
    "    override fun iterator(): Iterator<Data> = object : Iterator<Data> {\n",
    "        var loc = 0\n",
    "        override fun hasNext(): Boolean = loc < n\n",
    "        override fun next(): Data {\n",
    "            require(hasNext())\n",
    "\n",
    "            val start = loc\n",
    "            val end = min(loc + batchSize, n)\n",
    "            val f = features.slice(start, end)\n",
    "            val l = labels.slice(start, end)\n",
    "            loc = end\n",
    "            \n",
    "            // The actual data for this interation is stored in a Data object\n",
    "            return Data(f, l)\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Data Iterator\n",
    "\n",
    "Even though the number of data points is 100, the batchSize in the iterator will default to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dataIterator = SimpleDataIterator(trainingData.features, trainingData.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Linear Regression Model\n",
    "\n",
    "`LinearRegresson` is a single layer neural network using an __[AffineTransform](https://github.com/facebookresearch/diffkt/blob/main/kotlin/api/src/main/kotlin/org/diffkt/model/AffineTransform.kt)__ layer. The `AffineTransform` layer takes an input tensor, $features$, an estimated weight tensor, $m$, and an estimated bias tensor, $b$ and calculates the estmated tensor $labels$, where $labels = m * features + b$ using tensor operations. `LinearRegression` inherits from __[Model](https://github.com/facebookresearch/diffkt/blob/main/kotlin/api/src/main/kotlin/org/diffkt/model/Model.kt)__. `Model` is an abstract class, so `layers`, `withLayers()`, `hashCode()`, and `equals()` have to be implemented. `DTensor` and `DScalar` are like vals in Kotlin, they are fixed in value once they are initialized. In the linear regression model you want to learn the estimated weight and estimated bias in the model. To have a tensor that can be modified in the learning process, use a __[TrainableTensor](https://github.com/facebookresearch/diffkt/blob/main/kotlin/api/src/main/kotlin/org/diffkt/model/TrainableTensor.kt)__ instead. The estimated weight $m$ and the estimated bias $b$ are set as a `TrainableTensor`, initialized to a random value. If the learning is sucessful, the estmated weight and estimated bias will approximate the true weight and true bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "// Linear Regression\n",
    "\n",
    "class LinearRegression(val l: AffineTransform): Model<LinearRegression>() {\n",
    "    \n",
    "    constructor(m: DScalar, b: DScalar) : this(AffineTransform(TrainableTensor(m), TrainableTensor(b)))\n",
    "    constructor(random: Random) : this(FloatScalar(random.nextFloat()), FloatScalar(random.nextFloat()))\n",
    "\n",
    "    override val layers: List<Layer<*>> = listOf(l)\n",
    "\n",
    "    override fun withLayers(newLayers: List<Layer<*>>): LinearRegression {\n",
    "        require(newLayers.size == 1)\n",
    "        val newLayer = newLayers[0] as AffineTransform\n",
    "        return LinearRegression(newLayer)\n",
    "    }\n",
    "\n",
    "    override fun hashCode(): Int = combineHash(\"LinearRegression\", l)\n",
    "    override fun equals(other: Any?): Boolean = other is LinearRegression &&\n",
    "            other.l == l\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val linReg = LinearRegression(random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loss Function\n",
    "\n",
    "The loss function is necessary for gradient descent optimization. The loss function is a L2 loss function, which is the sum of the squared differences between the predicted value of a label and the actual value of a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun lossFun(predictions: DTensor, labels: DTensor): DScalar {\n",
    "    val diff = predictions - labels\n",
    "    return (diff * diff).sum()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "The optimizer uses the built in __[FixedLearningRateOptimizer](https://github.com/facebookresearch/diffkt/blob/main/kotlin/api/src/main/kotlin/org/diffkt/model/FixedLearningRateOptimizer.kt)__. It implements a simple gradient descent optimization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val optimizer = FixedLearningRateOptimizer<LinearRegression>(0.5F / trainingDataSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learner\n",
    "\n",
    "A Learner class needs to be created to train the model. At this point in time it does not inherit from a class in the model directory and needs to be implemented from scratch. The `modelTrainStep()` function does the actual training. It calculated the derivatives of the model and passes them on to the optimizer to be used in the gradient descent algorithm. The `optimizeModel` function generates a batch of data to pass to the `modelTrainStep` for the number of epochs in the `train()` function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Learner<T : Model<T>>(val batchedData: Iterable<Data>,\n",
    "                            val lossFunc: (predictions: DTensor, labels: DTensor) -> DScalar,\n",
    "                            val optimizer: Optimizer<T>) \n",
    "{\n",
    "    var totalTime = 0L\n",
    "\n",
    "    /**\n",
    "     * Trains the given model on the data set, for [epochs] epochs processing the data of the [dataIterator] in\n",
    "     * batches of size [batchSize].  Returns the trained model.\n",
    "     */\n",
    "    fun train(model: T,\n",
    "              epochs: Int,\n",
    "              printProgress: Boolean = false): T \n",
    "    {\n",
    "        \n",
    "\n",
    "        // The model training step function, which could possibly be optimized.\n",
    "        fun modelTrainStep(model2: T, batch: Data): Pair<DScalar, T> \n",
    "        {\n",
    "            val (loss, tangent) = primalAndReverseDerivative(\n",
    "                x = model2,\n",
    "                f = { model3: T ->\n",
    "                    val output = model3.predict(batch.features)\n",
    "                    val loss = lossFunc(output, batch.labels)\n",
    "                    loss\n",
    "                },\n",
    "                extractDerivative = { model3: T,\n",
    "                                      loss: DScalar,\n",
    "                                      extractor: (input: DTensor, output: DTensor) -> DTensor ->\n",
    "                                                model3.extractTangent(loss, extractor)\n",
    "                }\n",
    "            )\n",
    "\n",
    "            val trainedModel: T = optimizer.train(model2, tangent)\n",
    "            return Pair(loss, trainedModel)\n",
    "        }\n",
    "        \n",
    "        // batch management \n",
    "        val optimizedModel = (0 until epochs).fold(model) { model1: T, e: Int ->\n",
    "            var lossTotal: DScalar = FloatScalar.ZERO\n",
    "            \n",
    "            val trainedModel = batchedData.fold(model1) { model2: T, batch: Data ->\n",
    "                \n",
    "                // get some data\n",
    "                val batchOnDevice = batch.to(Device.CPU)\n",
    "                \n",
    "                // time the training step\n",
    "                val t1 = System.nanoTime()\n",
    "                val (loss, trainedModel) = modelTrainStep(model2, batchOnDevice)\n",
    "                val t2 = System.nanoTime()\n",
    "                totalTime += t2 - t1  \n",
    "                \n",
    "                if (printProgress) lossTotal += loss\n",
    "                trainedModel\n",
    "            }\n",
    "            \n",
    "            if (printProgress && ((e % 10) == 0)) println(\"Epoch $e Cumulative Loss: $lossTotal\")\n",
    "            trainedModel\n",
    "        }\n",
    "        \n",
    "        return optimizedModel\n",
    "    }\n",
    "\n",
    "    private fun e(n: Long) = n / 1e9f\n",
    "\n",
    "    fun dumpTimes() {\n",
    "        println(\"running time:  ${e(totalTime)} sec\")\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model\n",
    "\n",
    "An instance of the `Learner` is created and `train()` is called for 230 epochs.\n",
    "\n",
    "The loss is displayed every 10 epochs. The total training time is displayed.\n",
    "\n",
    "You can compare the trueWeight and the estimated trueWeight.\n",
    "\n",
    "You can compare the trueBias to the estimated trueBias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Cumulative Loss: 453.91916\n",
      "Epoch 10 Cumulative Loss: 17.30477\n",
      "Epoch 20 Cumulative Loss: 5.533401\n",
      "Epoch 30 Cumulative Loss: 1.7693163\n",
      "Epoch 40 Cumulative Loss: 0.56576484\n",
      "Epoch 50 Cumulative Loss: 0.18091545\n",
      "Epoch 60 Cumulative Loss: 0.057852346\n",
      "Epoch 70 Cumulative Loss: 0.018499354\n",
      "Epoch 80 Cumulative Loss: 0.005913419\n",
      "Epoch 90 Cumulative Loss: 0.0018901364\n",
      "Epoch 100 Cumulative Loss: 6.049421E-4\n",
      "Epoch 110 Cumulative Loss: 1.9333517E-4\n",
      "Epoch 120 Cumulative Loss: 6.179842E-5\n",
      "Epoch 130 Cumulative Loss: 1.9667383E-5\n",
      "Epoch 140 Cumulative Loss: 6.244504E-6\n",
      "Epoch 150 Cumulative Loss: 2.044776E-6\n",
      "Epoch 160 Cumulative Loss: 5.9208685E-7\n",
      "Epoch 170 Cumulative Loss: 1.5495567E-7\n",
      "Epoch 180 Cumulative Loss: 4.2282977E-8\n",
      "Epoch 190 Cumulative Loss: 2.2936376E-8\n",
      "Epoch 200 Cumulative Loss: 1.7647608E-8\n",
      "Epoch 210 Cumulative Loss: 1.7243167E-8\n",
      "Epoch 220 Cumulative Loss: 1.7243167E-8\n",
      "\n",
      "running time:  0.7214423 sec\n",
      "\n",
      "trueWeight = 5.0, estimated trueWeight = 4.9999514\n",
      "trueBias = 2.0, estimated trueBias = 2.0000231\n"
     ]
    }
   ],
   "source": [
    "// Build the model\n",
    "\n",
    "val learner = Learner(batchedData = dataIterator,\n",
    "                      lossFunc = ::lossFun,\n",
    "                      optimizer = optimizer)\n",
    "\n",
    "val trainedModel = learner.train(linReg, 230, printProgress = true)\n",
    "\n",
    "println()\n",
    "learner.dumpTimes()\n",
    "println()\n",
    "println(\"trueWeight = ${trainingData.trueWeight}, estimated trueWeight = ${trainedModel.l.m.tensor.toString()}\")\n",
    "println(\"trueBias = ${trainingData.trueBias}, estimated trueBias = ${trainedModel.l.b.tensor.toString()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "nbconvert_exporter": "",
   "pygments_lexer": "kotlin",
   "version": "1.7.20-dev-1299"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
